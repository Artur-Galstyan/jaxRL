{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec92eb5f-a11f-41d0-875d-f08058a9c42b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from typing import Optional, Tuple\n",
    "from jaxtyping import PRNGKeyArray, Array, Float, PyTree\n",
    "import numpy as np\n",
    "import gymnax\n",
    "import rlax\n",
    "import optax\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b7cde327-bce8-48f2-973f-2f989d613111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def calculate_gae(\n",
    "    rewards: Array,\n",
    "    values: Array,\n",
    "    dones: Array,\n",
    "    gamma: float,\n",
    "    lam: float,\n",
    ") -> Array:\n",
    "    def body_fun(\n",
    "        carry: tuple[Array, Array], t: Array\n",
    "    ) -> tuple[tuple[Array, Array], None]:\n",
    "        advantages, gae_inner = carry\n",
    "        delta = rewards[t] + gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "        gae_inner = delta + gamma * lam * (1 - dones[t]) * gae_inner\n",
    "        advantages = advantages.at[t].set(gae_inner)\n",
    "        return (advantages, gae_inner), None\n",
    "\n",
    "    values = jnp.append(values, values[0])\n",
    "    advt = jnp.zeros_like(rewards)\n",
    "    gae = jnp.array(0.0)\n",
    "    t = len(rewards)\n",
    "\n",
    "    (advt, _), _ = jax.lax.scan(body_fun, (advt, gae), jnp.arange(t - 1, -1, -1))\n",
    "    return advt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e0bb8ef-536d-4966-a524-f16d86794344",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rollout(rng_input, policy, env, env_params, steps_in_episode, epoch):\n",
    "    \"\"\"Rollout a jitted gymnax episode with lax.scan.\"\"\"\n",
    "    # Reset the environment\n",
    "    rng_reset, rng_episode = jax.random.split(rng_input)\n",
    "    obs, state = env.reset(rng_reset, env_params)\n",
    "    \n",
    "    def policy_step(state_input, tmp):\n",
    "        \"\"\"lax.scan compatible step transition in jax env.\"\"\"\n",
    "        obs, state, rng = state_input\n",
    "        rng, rng_step, rng_net = jax.random.split(rng, 3)\n",
    "        logits = policy(obs)\n",
    "        action = jax.random.categorical(logits=logits, key=rng_net)\n",
    "        log_prob = jax.nn.log_softmax(logits)[action]\n",
    "        next_obs, next_state, reward, done, _ = env.step(\n",
    "          rng_step, state, action, env_params\n",
    "        )\n",
    "        carry = [next_obs, next_state, rng]\n",
    "        return carry, [obs, action, log_prob, reward, done, state]\n",
    "\n",
    "    # Scan over episode step loop\n",
    "    _, scan_out = jax.lax.scan(\n",
    "      policy_step,\n",
    "      [obs, state, rng_episode],\n",
    "      (),\n",
    "      steps_in_episode\n",
    "    )\n",
    "    # Return masked sum of rewards accumulated by agent in episode\n",
    "    obs, action, log_prob, reward, done, states = scan_out\n",
    "    return obs, action, log_prob, reward, done, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6cdff524-8851-43a8-addb-b9d6cdbaa359",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RLDataset(Dataset):\n",
    "    def __init__(self, states, actions, rewards, log_probs, values, advantages, dones) -> None:\n",
    "        self.rewards = torch.tensor(rewards)\n",
    "        self.actions = torch.tensor(actions)\n",
    "        self.obs = torch.tensor(states)\n",
    "        self.dones = torch.tensor(dones)\n",
    "        self.log_probs = torch.tensor(log_probs)\n",
    "        self.values = torch.tensor(values)\n",
    "        self.advantages = torch.tensor(advantages)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.rewards)\n",
    "\n",
    "    def __getitem__(\n",
    "        self, idx\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor,\n",
    "               torch.Tensor, torch.Tensor, torch.Tensor, \n",
    "               torch.Tensor]:\n",
    "        return (\n",
    "            self.obs[idx],\n",
    "            self.actions[idx],\n",
    "            self.rewards[idx],\n",
    "            self.log_probs[idx],\n",
    "            self.values[idx],\n",
    "            self.advantages[idx],\n",
    "            self.dones[idx],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f04d032d-ef13-4ec3-a607-d26eacd2cee0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Actor(eqx.Module):\n",
    "    mlp: eqx.nn.MLP\n",
    "    def __init__(self, in_size: int,\n",
    "                 out_size: int,\n",
    "                 width_size: int,\n",
    "                 depth: int,\n",
    "                 *,\n",
    "                 key: PRNGKeyArray):\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=in_size,\n",
    "            out_size=out_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            key=key\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x: Array, key: Optional[PRNGKeyArray] = None):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class Critic(eqx.Module):\n",
    "    mlp: eqx.nn.MLP\n",
    "    def __init__(self, in_size: int,\n",
    "                 width_size: int,\n",
    "                 depth: int,\n",
    "                 *,\n",
    "                 key: PRNGKeyArray):\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=in_size,\n",
    "            out_size=1,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            key=key\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x: Array, key: Optional[PRNGKeyArray] = None):\n",
    "        return self.mlp(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "54cb9531-40ec-4f62-baf8-948618652e25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def get_value(obs: Float[Array, \"n_dims\"], critic: PyTree) -> Array:\n",
    "    return critic(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "21959c11-7143-4e0b-a1e1-571936dc48f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jit_rollout = eqx.filter_jit(rollout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "99df1087-d846-4ee4-99b1-cf9ba284b27d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def update_ppo(actor: PyTree,\n",
    "               actor_optimizer: optax.GradientTransformation,\n",
    "               actor_opt_state: optax.OptState,\n",
    "               critic: PyTree,\n",
    "               critic_optimizer: optax.GradientTransformation,\n",
    "               critic_opt_state: optax.OptState,\n",
    "               batch: Tuple[Array],\n",
    "               policy_clip: float = 0.2,\n",
    "              ):\n",
    "    print(\"JIT\")\n",
    "    obs, actions, rewards, old_log_probs, values, advantages, dones = batch\n",
    "    def ppo_objective(_actor, _obs, _old_log_probs, _advantages):\n",
    "        new_logits = eqx.filter_vmap(actor)(obs)\n",
    "        log_probs = jax.nn.log_softmax(new_logits)\n",
    "        new_log_probs = jnp.take_along_axis(\n",
    "            log_probs, jnp.expand_dims(actions, -1), axis=1\n",
    "        )\n",
    "        prob_ratio = jnp.exp(new_log_probs) / jnp.exp(old_log_probs)\n",
    "\n",
    "        weighted_probs = advantages * prob_ratio\n",
    "        weighted_clipped_probs = jnp.clip(prob_ratio, \n",
    "                                          1.0 - policy_clip, 1.0 + policy_clip) * advantages\n",
    "        clipped_objective = jnp.fmin(weighted_probs, weighted_clipped_probs)\n",
    "        return -clipped_objective.mean()\n",
    "    \n",
    "    def actor_step(_actor, _obs, _old_log_probs, _advantages, _actor_optimizer, \n",
    "                   _actor_opt_state):\n",
    "        grad = eqx.filter_grad(ppo_objective)(_actor, _obs, _old_log_probs, _advantages)\n",
    "        updates, _actor_opt_state = _actor_optimizer.update(grad, _actor_opt_state, _actor)\n",
    "        _actor = eqx.apply_updates(_actor, updates)\n",
    "        \n",
    "        return _actor, _actor_opt_state\n",
    "    \n",
    "    def critic_loss(_critic, _obs, _advantages, _values):\n",
    "        critic_values = eqx.filter_vmap(_critic)(obs)\n",
    "        returns = _advantages + _values\n",
    "        loss = returns - critic_values\n",
    "        return loss.mean()\n",
    "    \n",
    "    def critic_step(_critic, _obs, _advantages, _values, \n",
    "                    _critic_optimizer, _critic_opt_state):\n",
    "        grad = eqx.filter_grad(critic_loss)(_critic, _obs, _advantages, _values)\n",
    "        updates, _critic_opt_state = _critic_optimizer.update(grad, _critic_opt_state, _critic)\n",
    "        _critic = eqx.apply_updates(_critic, updates)\n",
    "        \n",
    "        return _critic, _critic_opt_state\n",
    "    \n",
    "    actor, actor_opt_state = actor_step(actor, obs, old_log_probs, advantages,\n",
    "                                        actor_optimizer, actor_opt_state)\n",
    "    \n",
    "    critic, critic_opt_state = critic_step(critic, obs, advantages, values, critic_optimizer,\n",
    "                                           critic_opt_state)\n",
    "    \n",
    "    return actor, actor_opt_state, critic, critic_opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "95725838-ef59-456c-a957-2425b30cbb45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_episodes = 1000\n",
    "n_epochs = 10\n",
    "policy_clip = 0.2\n",
    "batch_size = 64\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, key_reset, key_policy, key_step = jax.random.split(rng, 4)\n",
    "\n",
    "# Create the Pendulum-v1 environment\n",
    "env, env_params = gymnax.make(\"CartPole-v1\")\n",
    "jit_rollout = eqx.filter_jit(rollout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b0d89465-b357-4721-9bb9-352236c1c40a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key, actor_key, critic_key = jax.random.split(jax.random.PRNGKey(33), 3)\n",
    "actor = Actor(in_size=env.observation_space(env_params).shape[0], \n",
    "             out_size=env.action_space(env_params).n,\n",
    "             width_size=32,\n",
    "             depth=3,\n",
    "             key=actor_key)\n",
    "critic = Critic(in_size=env.observation_space(env_params).shape[0], \n",
    "             width_size=32,\n",
    "             depth=3,\n",
    "             key=critic_key)\n",
    "actor_optimizer = optax.adam(learning_rate=0.001)\n",
    "actor_opt_state = actor_optimizer.init(eqx.filter(actor, eqx.is_inexact_array))\n",
    "\n",
    "critic_optimizer = optax.adam(learning_rate=0.001)\n",
    "critic_opt_state = critic_optimizer.init(eqx.filter(critic, eqx.is_inexact_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6ec448b6-a249-4ae3-86d4-91de32ff05d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                  | 1/1000 [00:00<08:34,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 1000/1000 [00:46<00:00, 21.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 10  7 10  9 11  7 10  7 12  9  7 10  9 10  6 11  9  7  9  4  9 10  7\n",
      "  9  9  8  9  6  7 10 10  8  8  8  8 10 11  9 10  9 11  6  6 10 12 11  8\n",
      "  7  7 11 10  7  8  8  7  8 10  6  8 10 10  9  9  9  8  7 10  9 10  9  7\n",
      "  6  6  7  6 10  8  9 10 10  9  6  5 12  8  7  8  6  9  9 12 10 12  9  7\n",
      " 11  9  8 10  8 10  9  8  8  7  6  7  6  9  5  6  6 11 12  7 10 12  6  7\n",
      "  9 10  6  9  7 11  8 11  7  5 10  7  9 10  8 10 11  9 10 10  6  6  5 11\n",
      "  8  8  9 11  7 10  8 11  7  7  5  9  8  7 10  8  9 11  8 11  6 10  5  9\n",
      "  8  8  8 10  6 10  9  8  9 10  6  8 10 10 10  8 11  7  8  8  8 13  7 10\n",
      " 10  7  9  6  8  7 10  7 10  7  7  9  8 10 10  9 10  8 10  7  8 11 10 10\n",
      "  9  6  9  8 10  5 11 10  9 11  8  7  8  8  6  9  9 10 10  9  9  7  8  6\n",
      " 10  8 10  9  7  7  8  8  9 10  8  9  9  7  7 10  8  8 10  9  9 11 11  8\n",
      " 10  8  9  8  9  6  8  8 11  9  7  8  8 11  6  8 12 10  7  7  7  6  9  7\n",
      "  9  8 10 10  8  9  7  7 12  8  9  9 11 12 13 10  9  9  7  7  7  7  8  8\n",
      "  8  9  9  9  9  8 10  7 11 10  9  5  9 11  8  7  9  8  7  8 10  8  7  7\n",
      "  9  8  9 10 10  9 10  9 10  6  7 10  8  9  7  7  8 10  8  9  7  8  9 10\n",
      " 10  8 10  9 10  9  7  8  9  9 11  9  7 11  9  8 10 12 10  8  8  7 11  9\n",
      "  8 10 10  9  9 10  7  9  8 10  9 12  9 10  9 11  9 10 10  9 11  9  9  9\n",
      "  9  8  8 10  7 11  8  7  8  8  7 11 10 12  8  9  7 10  9 11  9 12  8  8\n",
      " 10 12  6 11  7  7 10  9  9 10  9  8 10  9  8  7  8  5 10 10 10  7  8  8\n",
      " 11  8 12 10  8  7  9  8 11  9 10 11  8 10  8  9  7  9  9 10 10  8 11  7\n",
      "  7 13  9 10  9  9  7  8 10  7  9  9 10  5  8  7 10  5  8  9  9  6 12  9\n",
      "  9  9  9  7  7  8 11 11  9  9 10 10 10  8  7  8  5  7  8 10  8  8 10  8\n",
      " 12 10  6  7  9  9  8  6  8  8  9 10  6 10  7  9  7  7  8  8  9 11  9  9\n",
      " 10  6  9 10  9  9  6 11 11 10 10  9  9  9  6  8  9 10 11  9  9  8  6  5\n",
      " 10  9 11  8  8 11  5  8 10 11  9  8  8  9  7  6  9  9  9  9 10 11 11  8\n",
      "  8  7 10  9  8  5  6  7 10  7  8  5  9  8  8  9  8 10 10  9  8  9  9  7\n",
      "  7  9 10  7  8  9  9  8 10  8  9  7 11 11  9  8  9  6  8 10  7  7  8  7\n",
      " 10  6  8 13  8  8  9  8  8 10 11 11  9 11  8  8  5  9  5 10  8 11 10  9\n",
      "  9  8 12  8 11 10  9  7  9 10 10  7  7 12  8  7  8  9  9  7  7 10  9  9\n",
      "  7 10  7  9  9  8 11 10  9  8  7  9  7 11  7  9  5 10  9  8 10  9 12  5\n",
      "  7 10 10  8  7 12  9 13  9 10 11 12  7 11 12  6  9  9  7  9 10  8  7 11\n",
      "  7  9 10 10  8  9  9 10  8  7 10 11  9  6  8  8  9 10  7  7  8  7  9 10\n",
      " 10  9  9  8  7  8 12 11  8  8  9  6 11  8 10  9 11  9  8  8 10  7  6  8\n",
      " 10  9 11  9  7  7  9  6 10  7  7  8  8  9  6 10 10 10  9  9  7 12 11 10\n",
      " 10 10  8  9  6  8  8  8  9  8  6  8  8  9 10 11  9  8  7  7 11  8  8  9\n",
      "  8  7  9  9  7 11  8  8  9 11 10  8  8 10 10 10  8  6  8 12 10  8 10  9\n",
      "  9  8  9  6  8  7  7 10 10 10 10  6 10  7  6 11  5  8  7 10  6  8 10 11\n",
      "  8  6  7  8 10  6  7  8  8  8 10 10 11  9  5  8 10 12  9  8  8  7 11 11\n",
      "  8 12  7  8  9  7  8  8 10  7  8 10  9  9  9  8 10 12  7 11  6  7  6  8\n",
      "  9  5 10  8  9  8 10 11  8 10  9 10  7 10 11  8  6  8  9  9  8  9  7  8\n",
      "  9 11  9 10  9  6 10  9  8  9  7 10  7 11  9  7  8  5  9 11  8 11  6 11\n",
      "  8  6 11  9  8 10  6  7  8  8  9 10 11 11  6  9]\n"
     ]
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(33)\n",
    "all_rewards = []\n",
    "for eps in tqdm(range(n_episodes)):\n",
    "    rng, subkey = jax.random.split(rng)\n",
    "    obs, actions, log_probs, rewards, dones, states = jit_rollout(subkey, actor, env, env_params, 200, 0)\n",
    "    \n",
    "    #true_indices = jnp.where(dones)[0]\n",
    "    #distances = jnp.diff(true_indices, prepend=-1)\n",
    "\n",
    "    # Calculate average distance\n",
    "    #avg_distance = jnp.mean(distances)\n",
    "    #all_rewards.append(avg_distance)\n",
    "    \n",
    "    all_rewards.append(jnp.sum(dones))\n",
    "    \n",
    "    values = jax.vmap(get_value, in_axes=(0, None))(obs, critic)\n",
    "    advantages = calculate_gae(\n",
    "        rewards = rewards,\n",
    "        values = values,\n",
    "        dones = dones,\n",
    "        gamma = 0.99,\n",
    "        lam = 0.95\n",
    "    )\n",
    "    dataset = RLDataset(\n",
    "        rewards=np.array(rewards),\n",
    "        states=np.array(obs),\n",
    "        actions=np.array(actions),\n",
    "        log_probs=np.array(log_probs),\n",
    "        dones=np.array(dones),\n",
    "        values=np.array(values),\n",
    "        advantages=np.array(advantages)\n",
    "    )\n",
    "    dataloader = DataLoader(batch_size=batch_size,\n",
    "                        shuffle=True, drop_last=True,\n",
    "                        dataset=dataset)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in dataloader:\n",
    "            obs, actions, rewards, old_log_probs, values, advantages, dones = batch\n",
    "            b = (\n",
    "                jnp.array(obs.numpy()),\n",
    "                jnp.array(actions.numpy()), \n",
    "                jnp.array(rewards.numpy()),\n",
    "                jnp.array(old_log_probs.numpy()),\n",
    "                jnp.array(values.numpy()),\n",
    "                jnp.array(advantages.numpy()), \n",
    "                jnp.array(dones.numpy())\n",
    "            )\n",
    "            actor, actor_opt_state, critic, critic_opt_state = update_ppo(actor, actor_optimizer, actor_opt_state, \n",
    "               critic, critic_optimizer, critic_opt_state, b,\n",
    "                                                                         policy_clip=0.2)\n",
    "\n",
    "print(jnp.array(all_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ca2ad-2481-430b-a5d0-8445c04764c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
